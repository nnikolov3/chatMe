{
    "file_path": "src\\files_to_process\\2312.07783v3.pdf",
    "file_hash": "071e5e7578d33a7b504fee1e9825f9d6daa5f4c060a09404dc146fd96deb924b",
    "processed_date": "2025-01-06T09:53:39.497585",
    "content": {
        "pages": [
            {
                "page_0": {
                    "ocr_text": "arX1v:2312.07783v3 [cs.CR] 1 Oct 2024\n\nBarraCUDA: Edge GPUs do Leak DNN Weights\n\nPeter Horvath\nRadboud University\n\nLejla Batina\nRadboud University\n\nAbstract\n\nOver the last decade, applications of neural networks have\nspread to every aspect of our lives. A large number of com-\npanies base their businesses on building products that use\nneural networks for tasks such as face recognition, machine\ntranslation, and self-driving cars. Much of the intellectual\nproperty underpinning these products is encoded in the exact\nparameters of the neural networks. Consequently, protecting\nthese is of utmost priority to businesses. At the same time,\nmany of these products need to operate under a strong threat\nmodel, in which the adversary has unfettered physical con-\ntrol of the product. In this work, we present BarraCUDA, a\nnovel attack on general-purpose Graphics Processing Units\n(GPUs) that can extract parameters of neural networks run-\nning on the popular Nvidia Jetson devices. BarraCUDA relies\non the observation that the convolution operation, used during\ninference, must be computed as a sequence of partial sums,\neach leaking one or a few parameters. Using correlation elec-\ntromagnetic analysis with these partial sums, BarraCUDA\ncan recover parameters of real-world convolutional neural\nnetworks.\n\n1 Introduction\n\nThe field of machine learning has seen an explosive increase\nin interest and use over the last decade. In particular, deep\nlearning has proven to be a versatile technique that provides\nstate-of-the-art performance for many real-world applications.\nThe use of Deep Neural Networks (DNNs) has proved useful\nfor a broad range of domains, including playing chess [57],\nobject detection [39], image classification [17, 26, 34, 38, 58],\naudio processing [49], forecasting [36, 52, 54, 55] and natural\nlanguage processing [46]. Thus, deep learning applications\nhave become indispensable and are changing many aspects\nof our everyday lives.\n\nDeep learning typically employs artificial neural networks\nconsisting of multiple layers of (simulated) neurons. When\ndesigning a deep learning solution for a problem, the de-\nsigner first chooses the network architecture, which specifies\n\nLukasz Chmielewski\nMasaryk University, Radboud University\n\nL\u00e9o Weissbart\nRadboud University\n\nYuval Yarom\nRuhr University Bochum\n\nthe layers of neurons, including their sizes, types, and acti-\nvation functions, as well as how the neurons are connected,\ni.e., Which neurons\u2019 outputs are connected to which inputs.\nThe designer then trains the network, selecting the weights\nused for each weighted sum and bias values that are added to\nthe sums prior to the computation of a non-linear activation\nfunction, such as rectified linear unit (ReLU) [23].\n\nTraining a network for any non-trivial example is a\nresource-intensive process. There is a need to curate a spe-\ncialized dataset of correctly labeled samples that can be used\nfor the training. Moreover, the training process often requires\ndays and even weeks of computation on specialized high-\nperformance hardware, such as large quantities of graphical\nprocessing units (GPUs), and the whole process requires spe-\ncialized expertise that is now in high demand. Thus, to protect\nowners\u2019 IP and to defend against potential attacks, trained\nmodels are often considered trade secrets, which should be\nprotected from undue disclosure.\n\nAt the same time, there is a substantial market incentive for\npushing machine learning to edge devices such as intelligent\ncameras, autonomous vehicles, and drones. Consequently,\ntrained models are being deployed under a threat model that\nallows adversarial physical access to devices and exposes\nthem to side-channel attacks. Indeed, side-channel attacks\nagainst neural network implementations on CPUs have been\ndemonstrated using both electromagnetic side-channel anal-\nysis [13] and microarchitectural attacks [62], among others.\nSimilarly, commercial deep-learning accelerators on FPGAs\nhave also been shown to be vulnerable to parameter extrac-\ntion via power-analysis [25]. However, GPUs are the dom-\ninant hardware in the world of deep learning due to their\nperformance and the software ecosystem that they provide to\nimplement neural networks. The CUDA parallel computing\nplatform allows developers to quickly and efficiently deploy\nDNNs on modern GPUs, whose applications have spread\nto many areas, from data centers to edge computing. How-\never, side-channel attacks on GPUs are challenging due to\ntheir complexity and inherent parallelism. So far, attacks on\nGPU implementations have only succeeded in recovering the\n",
                    "visual_analysis": "Caption: A novel attack on GPU networks using Deep Learning for attacks.\nTable 1 shows the sizes of neurons and their types used by CUDA parallel computing to recover parameters from real-world applications such as self-driving cars or drones. As shown in Table~\\ref{tab:table1}, a large number of these inputs are not connected, so an adversary can try different combinations using known sums for each network without knowing the specific neural networks being targeted.\nTable 2 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 3 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 4 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 5 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 6 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 7 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 8 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 9 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 10 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 11 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 12 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 13 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 14 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 15 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 16 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 17 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 18 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 19 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e.g., ResNet or Google's MobileNet) on a real-world application, an attacker can design sophisticated attacks for different applications using these input neurons' properties without specific knowledge of the target network.\nTable 20 shows that CUDA parallel computing is sensitive to parameters such as neuron sizes and types used by GPUs. By learning from training data with similar architectures (e"
                }
            },
            {
                "page_1": {
                    "ocr_text": "network architecture but not the parameters [16, 29, 41].\nTherefore, this work focuses on the following research\nquestion:\n\nAre proprietary implementations of neural networks on GPU\nvulnerable to parameter extraction using side-channel analy-\nsis?\n\nOur Contribution\n\nThis work answers the question affirmatively. We perform\nside-channel attacks on multiple GPU architectures, such as\nthe Nvidia Jetson Nano [9] and Nvidia Jetson Orin Nano [7],\nrecovering the weights and biases of real-world networks.\n\nSpecifically, our attacks collect multiple traces with ran-\ndom inputs and then use correlation electromagnetic analysis\n(CEMA) [14] adjusted to recover the model\u2019s parameters from\nthese traces. We further demonstrate the success of our attack\non different neural network layers, such as convolutional and\ndense layers with varying batch sizes.\n\nOur attack relies on the observation that convolution com-\nputation, the core operation in many neural network imple-\nmentations, cannot be computed at once. Instead, convolution\nis computed as a sequence of partial sums, each depending on\nthe previous sum and a small number of weights and inputs.\nConsequently, assuming the previous sum and the inputs are\nknown, the adversary can guess the weights, use these to pre-\ndict leakage, and correlate the prediction with the measured\nside-channel trace. However, to carry out the attack, we need\nto overcome several challenges.\n\nPartial Sums Identification: Convolutional and dense lay-\ners in neural networks can be implemented in multiple ways;\none of the most commonly used ways is matrix multiplica-\ntions [15]. In a differential SCA attack, knowing how the\ntarget algorithm is implemented is crucial. Therefore, we first\nreverse-engineer CUDA binaries produced by TensorRT to de-\ntermine both the parameter representation, the relevant partial\nsums, and the weights they depend on.\n\nAttack Localization: A second challenge is that for an\neffective attack, the attacker needs to localize the attack, both\nspatially and temporally. To overcome this challenge, we\nadapt techniques from the domain of side-channel attacks\non cryptographic implementations. We use a variant of Test\nVector Leakage Assessment (TVLA) [56] both for finding the\nbest physical location for placing the probe and for finding\nthe time during the operation of the neural network in which\na specific neuron is evaluated. Our leakage detection analysis\nproved effective for different GPU architectures.\n\nGPU architecture: The GPU inherently introduces a high\namount of noise due to its Single-Instruction-Multiple-Thread\n(SIMT) [10] architecture, where many parallel threads are exe-\ncuting simultaneously. Furthermore, due to the GPU hardware\nimplementation, the scheduling of threads is not guaranteed\nto be fixed in time, which makes SCA attacks (e.g., CEMA)\n\nmuch harder than on other platforms, such as FPGAs and\nmicrocontrollers, where the execution is deterministic and\nsequential. Moreover, unlike attacks on cryptographic imple-\nmentations, where the attacker typically aims to recover a\nrelatively short sub-key, we need to extract a large number\nof parameters for DNN model extraction. A large number of\ntraces and efficient signal processing are required to overcome\nthese challenges. To that end, we develop a CUDA-based im-\nplementation of CEMA to execute the attack an order of\nmagnitude faster for large datasets with millions of traces.\n\nParameter Extraction: We apply our techniques to con-\nvolutional and dense layers in neural networks while also\ninvestigating the impact of batch size on our attack. Our at-\ntacks against convolutional layers target a variation of the\nbaseline EfficientNet [59] model. We deploy the model on\nthe Jetson Nano in FP16 precision and the Jetson Orin Nano\nin INT8 precision for the parameters. We successfully and\nefficiently extract parameters from both GPUs with different\ndata types using our CUDA-based analysis tool. Overall, the\nattack against the Jetson Nano requires 10 days for collecting\ntraces and one to two days for trace alignment with an input\nbatch size of 1. On the other hand, the Jetson Orin Nano re-\nquires only 1 day for trace collection and one day for trace\nalignment with an input batch size of 1. An experiment with\na larger batch size was also conducted on the Orin Nano, and\nit took 5 days to collect traces and align them with an input\nbatch size of 16. Once aligned, parameters can be extracted\nat a rate of one weight in six minutes for FP16 and 5 minutes\nfor INT8 weights. The whole process is highly parallelizable.\nThus, attacks on moderate-size models are well within the\ncapabilities of well-resourced adversaries.\n\nDisclosure. We notified Nvidia of the vulnerabilities found,\nand they acknowledged our findings. Nvidia recommends\nthat users follow guidelines to prevent physical access and\ninformation leakage.\n\nOrganization. The rest of this paper is organized as follows:\nAfter providing the necessary background on side-channel at-\ntacks and deep learning (Section 2), we describe the overview\nof our attack and our experiment setup in Section 3. In Sec-\ntion 4 we describe our procedure for profiling the implemen-\ntation, identifying relevant partial sums, and localizing their\nleakage. Finally, we present our parameter extraction attack\nin Section 5. In addition, Section 6 covers the limitations,\npossible extensions, countermeasures, and related work.\n\n2 Background\n\nHere, we introduce the concepts of side channels and related\ntechniques used for the attacks, and we provide some back-\nground on Nvidia\u2019s CUDA programming model and GPU\narchitecture.\n",
                    "visual_analysis": "Text appears on the left side of a page and is too small to read but some words can be identified such as: network architecture; research question; GPU models; CNN model extraction; FPGAs & microcontrollers; deep learning algorithms; NVIDIA Jetson Nano 9, Nvidia Jetson Orin Nano. Other sections include Background, Our Contribution, Parameter Extraction Techniques (with bullet points), Partial Sums Identification Convolutional and DSA techniques used on neural networks using CUDA programming models and GPU."
                }
            },
            {
                "page_2": {
                    "ocr_text": "2.1 Deep Neural Networks\n\nDeep neural networks (DNN) are universal function approx-\nimators [28] that solve tasks by learning from data. First, a\nmodel learns from training data; then it can be deployed to\nmake predictions on new, unseen data. Two main components\ninfluence the trained model\u2019s performance on unseen data: the\narchitecture and the parameters. The architecture refers to the\nstructure of the model, the types, and order of transformations\nthat the model applies on its inputs to arrive at some output.\nThese transformations are also commonly called /ayers, and\ntheir output can be tweaked by changing their internal pa-\nrameters. During the training process, these parameters are\ntweaked so that the final output yields correct results.\n\nConvolutional layer. A convolutional layer consists of small\nkernels that extract different features from the layer\u2019s input\ne.g., an image. The dimensions of these kernels are usually\nmuch smaller than the input\u2019s dimensions to be able to extract\nfine-grained details. Each kernel extracts different features\nwith its own parameters: the weights and bias. Each of them\ncalculates the convolution between their parameters and a\nsmall part of the input by sliding every kernel on the input with\na certain step size. In this work, we demonstrate the extraction\nof parameters, the weights, and the bias from convolutional\nlayers.\n\nDense layers. A dense layer in a neural network consists of\nnodes where each node outputs the weighted sum of all the\ninputs. In this paper, we demonstrate the extraction of weights\nfrom dense layers.\n\n2.2 Side-Channel Analysis\n\nSide-Channel Analysis (SCA) exploits unintended physical\nleakages of electronic devices to extract secret information\nprocessed by them [32, 33]. Such leakage can occur through\nvarious channels, including power consumption, electromag-\nnetic emanations (EM), timing, optical, or sound. It might\nlead to leakage of various types of secret information, e.g., on\ndata and instructions processed. In academic settings, Side-\nChannel Analysis (SCA) was first introduced in the 90\u2019s,\ntargeting cryptographic implementations running on then\npopular but constrained cryptographic devices such as smart-\ncards [32, 33] and SCA poses ever since a constant threat to\nthe security of various embedded systems. In that context,\nSCA aimed to recover the secret keys used in the crypto-\ngraphic implementations. In this work, we exploit the EM\nside channel emanating from a GPU platform on which a\nneural network is running, but instead of targeting secret keys,\nwe show how to recover neural network secret parameters:\nweights and biases.\n\n2.2.1 Electromagnetic Emanation\n\nThe electromagnetic emanations from a computing device\ncorrelate with the code and data the device processes. This\ncorrelation has been used to break cryptographic implementa-\ntions [35, 51], reverse-engineer neural networks [13, 16] and\neavesdrop on display units [22, 27, 40].\n\nIn Correlation EM Analysis (CEMA) [14], the attacker uses\nthe correlation coefficient as a side-channel distinguisher, i.e.,\nthe statistical method used for the key recovery. Essentially,\nCEMA allows an attacker to recover parts of a secret that\nis used in a targeted operation by using a known plaintext\nattack: measured samples are correlated against a synthetic\nleakage value (i.e., leakage model) that is generated from an\nintermediate value calculated for all possible values of a (part\nof) the secret. In our case, all CNN parameters (i.e., weights\nand bias) are considered as the whole secret, and a single\nweight or bias is considered to be a single target of CEMA\nthat needs to be repeated for all of them; the intermediate\nvalues are results of computations within neurons. Observe\nthat for the above approach to work, we need to assume that\nthe inputs used in the computations are different and are\nknown to the attacker.\n\nTwo commonly used leakage models in SCA are the Ham-\nming weight (HW) model, which predicts that the leakage is\nlinear with the number of set bits in the data (i.e. its Hamming\nweight), and the Hamming distance (HD) model, which pre-\ndicts that the leakage is linear with the number of bits that flip\nbetween consecutive data values. HW leakage usually occurs\nin practice when a value is transferred via the system bus, and\nHD leakage occurs when an intermediate value stored in a\nregister is overwritten with another value. We consider both\nof these models in this paper.\n\n2.2.2 Leakage Assessment\n\nFor leakage assessment, which is a critical part of a security\nevaluation of a chip, we rely on the default techniques used\nin side-channel analysis, i.e., intermediate-value correlation\nand Test Vector Leakage Assessment (TVLA) [56]. The idea\nbehind intermediate-value correlation is to consider the cor-\nrelation traces generated for all possible values of the secret.\nThere should be a correlation peak in the trace when the\ncorrect guess (for the secret) is processed because the guess\nis then in agreement with the leakage predicted. This is a\nconsequence of the fact that processing different data causes\ndifferent physical information, such as timing, power, or EM,\noften referred to as leakage. While this approach is often used\nin actual side-channel attacks, the disadvantage is that it re-\nquires a large number of traces, similar to the CEMA attack.\nTVLA or other leakage detection methods, like x?-test [44],\nare often more efficient (faster) as we control the parameters\nin the leakage detection phase. The main idea of TVLA is to\ncheck whether two distributions that process different inter-\nmediate values are equivalent or not using Welch\u2019s t-test. If\n",
                    "visual_analysis": "The first paragraph discusses deep neural networks (DNNs), which are universal functions for solving tasks by learning from approximators [28]. It mentions that DNNs learn from training data and can be deployed to make predictions on new unseen data without influencing the model's performance in unknown states, thanks to a reference architecture. The text then shifts focus to electromagnetic emanation (EMAs) as used by attackers for recovering parts of secret information during the processing phase.\n\nThe second paragraph explains how convolutional layer analysis is employed to target specific kernels from an image using different features such as dimensions and weights. It describes the calculation method involving kernel weights, parameters, inputs on a convolutional network output within a certain step size. The paper also presents two commonly used models in SCA for analyzing electromagnetic emanations.\n\nThe third paragraph elaborates on side-channel analysis (SCA) to explore leakage of electronic devices by processing secret information and discussing how SCAs were first introduced in the 1990s, particularly targeting cryptographic implementations with popular but constrained cryptographic devices. The text mentions that SCA poses a significant threat as it enables attackers to target secure systems through various embedded systems.\n\nThe fourth paragraph explains side-channel analysis (SCA) exploitation for unintended information leakage using electromagnetic emanations from electronic devices processed by them [32, 33]. It describes the concept of intermediate-value correlation and test vector leakage assessment tools like CEMA. The text highlights that SCA was first introduced in academic settings to target cryptographic implementations.\n\nThe fifth paragraph discusses two commonly used models for SCAs within embedded systems: Hamming weight (HW) model and Hammering distance (HD) model, along with their respective features such as switching frequency during transmission or leakage values correlated by HD. It also introduces the EM side-channel emanating from a GPU platform in which neural networks are running.\n\nThe final paragraph focuses on demonstrating how to recover hidden secret information through SCA using different methods like CEMA and HW models for measuring data leaks, especially with power consumption as an indicator of sensitive processes during system execution."
                }
            },
            {
                "page_3": {
                    "ocr_text": "Jetson Nano Jetson Orin Nano\n\nGPU architecture Maxwell Ampere\n# SMs 1 8\n\n# CUDA cores 128 1024\n\n# Tensor cores - 32\n\nMax. clock 920 MHz 625 MHz\nFP16 Va ~\n\nINT8 - Va\n\nTable 1: Comparison between the GPUs of Jetson Nano and\nJetson Orin Nano.\n\nthe two groups are deemed equivalent, then the TVLA will\nnot observe the leakage. Concretely, we verify whether two\nsets of measurements show significant differences if one set\nhas a fixed weight and the other has random weights. We\nrefer to this setting as \u201cfixed versus random\u2019. Since TVLA\nsearches for any leakage not necessarily exploitable by an\nattack like CEMA, it also usually requires much fewer traces\nthan intermediate-value correlation. Therefore, for the sake of\nefficiency, we mainly use TVLA in this paper, especially for\ndetermining which part of the traces processes each weight\nusing TVLA. We use intermediate-value correlation only to\ndetermine the best location of our probe! and for preliminary\ncharacterization of leakage patterns.\n\n2.3 CUDA Programming Model\n\nTo leverage the parallelism offered by GPUs, Nvidia exposes\nthe CUDA programming model [5] to developers. In this\nmodel, multiple abstraction levels exist and each level has\ndifferent implications forGPU hardware. The lowest level of\nabstraction is the thread, which executes a CUDA function\ndefined by the developer. The number of threads executing\nthe CUDA function in parallel is specified at the time of\ninvoking the function.\u201d Subsequently, multiple threads can\nbe grouped together into a single block of threads. Threads\nin a block have a per-block on-chip shared memory region\nwhere they can exchange data with other threads. Blocks of\nthreads form a grid of thread blocks. Each block in a grid\nexecutes independently from other blocks, but all blocks in\nthe grid share the same off-chip global memory region.\n\n2.4 GPU Streaming Multiprocessor\n\nWhen a CUDA function is invoked, the parallel threads exe-\ncute on the GPU\u2019s Streaming Multiprocessor (SM). A GPU\ncan consist of one or more SMs to improve parallelism fur-\nther. In this paper, we mount our attack on two different GPU\n\n'We simply check at which location the absolute t-test peak is the highest\nas in [20].\n\n2Functions in CUDA are also called kernels. We use the term function to\navoid confusion with kernels in CNNs.\n\n' |\n1\n|DNN architecture Parameter\n\n[+> DIN parameters\ni\n\nFigure 1: High-level description of the attack procedure.\n\narchitectures, the Maxwell [11] and Ampere [7] GPUs em-\nbedded in a System-on-Chip (SoC). For both architectures,\nwhen blocks of threads are scheduled onto a particular SM,\nthe threads in the blocks are divided into groups of 32 threads,\nalso called warps. Every warp is assigned to a particular\nProcessing Unit (PU) in the SM, and the warp scheduler in\na PU is responsible for scheduling and issuing instructions\nfor warps that are ready for execution at every clock cycle.\nAdditionally, both architectures have 4 PUs per SM, each with\ndedicated resources (e.g., register file) to manage warps. This\nmeans that 4 warps can be issued instructions in parallel at\nevery clock cycle in an SM. The Jetson Nano and Jetson Orin\nNano\u2019s GPUs are similar, but significant differences are sum-\nmarized in Table |. Overall, the Ampere GPU is a larger and\nmore capable GPU with more hardware support for different\noperations (matrix-multiply) and data types (e.g. INT8) that\nare heavily used in deep-learning inference.\n\n2.5  TensorRT Workflow\n\nTensorRT is a framework dedicated to accelerating neural\nnetwork inference on GPU, using implementations from dif-\nferent libraries (CuDNN, CuBLAS, TensorRT) that are timed\nagainst each other to choose the fastest. We use the TensorRT\nframework in our experiments and demonstrate the parame-\nter extraction attack on the implementations provided by the\nframework.\n\n3 Attack Procedure\n\nOur attack, depicted in Figure 1, assumes that the adversary\nknows or can recover [16, 29, 41], the DNN model architec-\nture. The adversary aims to use side-channel observations to\nrecover the unknown weights and biases used in the victim\ndevice. The attack follows two main phases. In the first, the\nadversary uses their prior knowledge to instantiate an equiv-\nalent DNN architecture on a profiling device with identical\nhardware, albeit without the weights. This profiling device is\nused to learn how information about weights and biases leaks.\nThis knowledge is then used to guide the second phase of the\nattack, which recovers the desired information.\n\nThe attack builds on the observation that convolution is the\ncore operation performed during inference. Specifically, A\nsingle p x g kernel with weights w, bias b and input feature\n",
                    "visual_analysis": "Table 1 compares the Jetson Nano and Jetson Orin Nano GPUs by listing their specifications such as GPU architecture Maxwell Ampere, number of SMs (Stream Processing Units), CUDA cores, TensorFlow support, maximum clock speeds for CPU core and memory controller, INT8 compatibility, FP16 support. The table highlights that both are capable of supporting various machine learning tasks including reinforcement learning with the use of deep residual networks in different contexts like CEMA or CEBA.\n\nThe figure caption indicates a focus on high-level descriptions of attack procedures within certain architectures (Maxwell and Ampere), specifically Jetson Nano, OrinNano. It suggests that these GPU models are part of broader research efforts to analyze vulnerability under various conditions such as fixed versus random weight changes in machine learning tasks like CEMA or CEBA.\n\nThe text elaborates on the limitations and challenges associated with detecting vulnerabilities (e.g., not observing leakage during testing) by highlighting specific techniques used for verification, including checking whether two sets of measurements show significant differences. It discusses historical approaches to detection such as CVLA and their potential inaccuracies due to random weight changes in training.\n\nThe CUDA programming model is introduced along with its associated challenges like GPU exposure to different levels of abstraction which can influence performance characteristics (like the number of threads, block dimensions). The text also mentions various attack procedures used by adversaries during machine learning inference or hardware exploitation. It discusses the use of profiling devices for understanding information about weights and biases.\n\nFinally, it describes TensorRT's role as a framework enabling GPU-accelerated neural network inference on GPUs using different libraries like CuBLAS, TensorRT that utilize specific kernels (e.g., p x q kernel with weight w, bias b) to avoid confusions in CNNs."
                }
            },
            {
                "page_4": {
                    "ocr_text": "map x calculates as follows:\n\nPq\nYi wi: xi Sw Xp te +Wy.g*Xpq, CO)\ni=l\n\nCout = Csum +b, (2)\n\nCsum = X*W =\n\nwhere Csym is the result of the convolution between the\nweights and the input, while c,,; 1s the output of the kernel\n(if the kernel contains a bias). Finally, if the layer includes an\nactivation function f then it is applied after the convolution:\n\ncCr= f (Cour):\n\nSince the convolution in Equation | cannot be computed\nin a single step, its computation consists of a series of in-\ntermediate computations, each depending on one on a small\nnumber of weights. Our attack targets these intermediate com-\nputations, looking for correlations between the results of the\ncomputations and the side-channel observations. We refer to\nthese results as intermediate values.\n\nThe profiling phase aims to identify the intermediate val-\nues used in the implementation and to /ocalize at which time\npoints they leak in the traces. The attack then performs correla-\ntion analysis on the side-channel information at the identified\nlocations to recover the weights.\n\nThe rest of this section outlines our threat model and de-\nscribes the experimental setup. The attack itself is described\nin two sections. First, Section 4 describes the profiling phase.\nThen, Section 5 describes and evaluates the recovery of\nweights and biases from the victim device.\n\n3.1 Threat Model\n\nOur attack targets edge devices that execute machine learning\ninference for their functionality. The attacker aims to recover\nthe trade secrets encoded in the parameters (weights and\nbiases) of the machine learning model, for example, in order\nto steal the IP that encodes them or as a step in designing an\nadversarial attack on the machine learning model.\n\nWe assume that the target device operates correctly and\nthat the code is secure so the attacker cannot exploit program-\nming vulnerabilities to acquire the parameters. However, we\nassume that the attacker knows the architecture of the model,\nincluding the number of layers, their sizes and types, and how\nthey interconnect. Attackers that do not have the information\ncan use techniques developed in past works to recover the\narchitecture [16, 29, 41].\n\nAs is typical for edge devices, we assume the attacker has\nunfettered physical access. In particular, we assume the at-\ntacker can open the device and place electromagnetic probes\nat locations that leak information. As we discuss in Sec-\ntion 3.3, our target devices have multiple leaky locations,\nallowing the attacker a choice.\n\nLast, we assume that the attacker can monitor the electro-\nmagnetic emanations from the target device during the time\n\nthat the device performs inference. The attacker needs to be\nable to observe the emanations over multiple sets of inputs\nand should also be able to choose these inputs if the target\nparameters have INTS8 data to reduce complexity. Otherwise,\nFP16 parameters do not require control of the inputs.\n\n3.2 Sensitive Intermediate Values\n\nA successful CEMA attack against a target algorithm requires\nthe attacker to find sensitive intermediate values that depend\non secret data. In convolutional and dense layers, a sensible\nchoice for these intermediate values is the partial sums that\ndepend on the secret weights. The partial sums allow an ad-\nversary to attack one or a few weights at a time and, therefore,\nto reduce complexity. If an attacker targeted the final results\nof these layers, the complexity would increase as these results\ndepend on many secret weights, making the attack infeasible.\n\nHowever, the actual implementation of these layers, and\nsubsequently the computation of partial sums, are dependent\non the target layer and hardware characteristics. Some of the\nimportant layer characteristics are the number of input and\noutput channels in a layer and the used representation for the\nparameters. In addition, hardware characteristics such as the\nnumber of SMs and shared and global memory size can also\ninfluence this implementation. Consequently, different layer\nand hardware configurations can lead to slightly different\nimplementations. Although these implementations can have\nidentical structures, fine-grained details such as partial sum\ncomputations can differ. Therefore, an attacker first needs\nto reverse engineer how each layer of the target DNN archi-\ntecture is implemented on the target device. This task can\nbe aided via reverse-engineering of the GPU framework and\nanalysis of leakage of the profiling device.\n\n3.3 Experimental Setup\n\nTo gather side-channel information, we collect EM traces as\nthey are less invasive and can provide more localized infor-\nmation than power measurements. It is also closer to the real\nworld as fewer modifications to the chip are required. Our two\ntargets, the Jetson Nano and Jetson Orin Nano, are similar in\nthat both feature a SoC mounted onto a PCB in a flip-chip\npackage. However, the Jetson Orin package is significantly\nlarger than the Jetson Nano\u2019s while also surrounded by more\ncapacitors. In order to access the packages, we remove the\nheatsinks from both devices and use an external fan to provide\ncooling to the devices. In our setups, the GPU cores operate at\nthe highest supported clock frequencies as shown in Table 1.\nWe use the Lecroy 8404M-MS oscilloscope at a sampling rate\nof 10GS/s with a Langer MFA-R 0.2-75 near-field probe [1]\n\n3One can also imagine an attacker without a profiling device searching\nthrough all the possible intermediate values to find the correct one. This\napproach is feasible but time-consuming, and we do not follow it.\n",
                    "visual_analysis": "The text describes a complex process involving convolution equations and various inputs such as weights, biases, and computational results. It discusses the importance of intermediate values for device performance inference and introduces concepts like \"c_out\" related to convolution models.\n\nA detailed breakdown includes:\n- Convolution operations using variables like \\( x \\), \\( w \\), etc.\n- Outputs from convolution involving summing products\n- Intermediate results (weights, biases) computed in a single step\n- The role of target devices for computing these values\n\nThe text also mentions \"threat model\" and the use of electromechanical probes to detect information leakage. It refers to various models such as Jetson Nano and DNN architectures.\n\nLastly, it discusses EM traces on edge devices like Jetson Nanos, highlighting their security vulnerabilities due to memory usage differences between hardware components (CPU, GPU)."
                }
            },
            {
                "page_5": {
                    "ocr_text": "(a) Location of Langer EM\nprobe between two capaci-\n\ntors on the Jetson Nano.\n\n(b) Location of Langer EM\nprobe between 3 capacitors\non Jetson Orin Nano.\n\nFigure 2: EM probe locations for the Jetson Nano and Jetson\nOrin Nano devices for successful parameter extraction attacks.\n\nto collect electromagnetic traces. In our experiments, we find\nthat we need a sampling rate of at least 5 GS/s to see leakage.\nIncreasing the frequency up to 20 GS/s does not improve the\nresults. In the FP16 case, the number of samples in a single\ntrace for the first convolutional layer is 400 000 with a batch\nsize of 1. In the INT8 case, it is 150K per trace.\n\n3.3.1 EM Probe Positioning\n\nTo find the best position for the EM probe, we use both TVLA\nand intermediate-value correlation experiments. Specifically,\nwe instantiate models whose architectures are identical to the\ntarget model for each possible location. We then capture two\nsets of traces. The weights and inputs are always the same\nin the \u201cfixed\u201d set of traces. In the \u201crandom\u201d set of inputs, we\nselect a random value for one of the weights in the model,\nand the inputs are the same. We then use TVLA to measure\nthe statistical difference, expressed as the t-value, between\nthe set of traces and intermediate-value correlation to find\nthe correlation between the random weights and the leakage.\nSignificant correlation and t-value peaks indicate a strong\nsignal.\n\nJetson Nano We observe several promising locations for\nplacing the EM probe. Figure 2a shows one such location\nbetween two capacitors in the power-supply circuit of the\nJetson Nano. Additionally, using XY scan, we find several\nlocations that exhibit similar leakage on the surface of the\nJetson Nano\u2019s SoC. Parameter extraction is possible from both\nthe best location on the SoC and between the capacitors. In\nthe rest of this paper, we use the capacitors\u2019 location because\nprobe placement is easier and does not require a detailed scan.\n\nJetson Orin Nano On the Jetson Orin\u2019s chip surface, we\ncannot pick any GPU-related signal. However, some nearby\ncapacitors still leak information related to GPU activity. Fig-\nure 2b shows one vulnerable spot that allows for parameter\nextraction. With further experimenting over the chip surface,\nwe find that probes sensitive to higher frequencies, such as the\nLanger RF-B 0.3-3 [2], can pick up exploitable signals over\nthe chip as well. In this paper, the presented results use the\nlocation shown in Figure 2b with the Langer MFA-R 0.2-75\nprobe.\n\n3.3.2 Trace Acquisition\n\nIn order to extract the parts of the traces related only to the\ninference operation, we used nsys from the CUDA Toolkit\nto get information about the execution times of the operations\non the GPU. Therefore, we used the Lecroy oscilloscope\u2019s\nSmartTrigger feature to trigger on the rising edge of the first\nlayer as it proved to be more reliable.\n\n3.3.3 Trace Preprocessing\n\nIn general, the collected traces contain a lot of jitter and the\nclock of the cores is not stable, which can be confirmed by\nlooking at the traces in the frequency domain. This makes\nthe detection of time where the implementations leak and the\nsubsequent CEMA attack harder. Since aligning the traces\naccurately with static alignment [42]* at many locations at\nthe same time is not possible, we use elastic alignment [60]\nto improve the leakage detection process. Although elastic\nalignment performs alignment at every time point, it comes\nat a price: it requires finding the optimal input parameters\nand is computationally expensive. Moreover, despite tuning\nthe parameters, it decreases the amount of leakage in the\ntraces. Therefore, it is used only to detect leaking points, but\nthe CEMA attack is carried out on the raw traces after static\nalignment is applied on the leaky part of the trace.\n\n4 Profiling\n\nWe now turn our attention to the profiling phase of the attack,\nin which the adversary uses a profiling device to analyze\nthe model and identify trace positions in which partial sums\nleak. Profiling consists of two main steps. In the first step,\nthe adversary analyzes the software that the GPU executes to\nidentify partial sums that depend on weights and characterize\nthe dependencies. Once these are found, in the second step,\nthe adversary collects side-channel traces from the profiling\ndevice and uses statistical tools to find the leaking time points\nin the traces at which leakage of each weight and bias can be\nobserved.\n\n4.1 Layer Implementation\n\nIn this section, we discuss the high-level structure of the code\nthat performs the operations of convolutional and dense lay-\ners in DNN models. As the code is unavailable to us, we use\ncuobjdump [6] to produce assembly code, which we can\n\n4Static alignment employs a standard pattern-based approach: we select a\npart of a trace as a reference and compute correlation for each offset within a\nchosen range for each of the traces. We then shift each trace by the respective\noffset that maximizes the correlation.\n\n>Elastic alignment is a parametrized machine-learning-based technique to\nalign the traces on all the distinctive patterns at the same time. However, this\nmethod tends to be error-prone and results in a decreased leakage in practice.\n",
                    "visual_analysis": "EM probe locations for the Jetson Nano and Jetson Orin Nano devices are shown on a figure with two sub-figures labeled (a) and (b). Sub-figure (a), \"Location of Langer EM probe between two capacitor probes on the Jetson Nano,\" shows an overhead view, likely through microscopy or similar technique. It depicts what appears to be electronic components inside the device's casing.\nSub-figure (b), \"Location of Langer EM probe between three capacitors on the Jetson Orin Nano,\" is a side profile image showing two parallel probes positioned close together over several capacitors within an electronic component package, with one labeled '1', and another numbered as '2'. The text beneath this figure indicates that these locations are for successful parameter extraction attacks.\nThese figures likely support or relate to the information in the accompanying paragraph about EM probe locations on Jetson Nano devices."
                }
            },
            {
                "page_6": {
                    "ocr_text": "analyze. While each implementation is different, all convolu-\ntional implementations follow the same structure:\n\n\u00b0 @) block of initialization instructions,\n\n\u00b0 Q) block of convolution operations, and\n\n\u00b0 GB) block of bias addition and ReLU calculation.\n\nWe now explain these three blocks in more detail.\n\n@) Init Block. The first main block consists of instructions\nthat set up the CUDA function. This block initializes the 64\naccumulator registers that are later used to store the convolu-\ntion results (RO\u2014-R63). Higher registers (R64 and above) are\nused to load the weights and inputs. Since the GPU registers\nare 32-bit wide, each higher register is loaded with either two\nFP 16 values or four signed 8-bit integers, depending on the\ndata type used for the computations.\n\n(2) Convolutional Block. The second block performs the\nconvolution of the weights and the inputs, i.e., the partial sums\nare computed in this block. It consists of repeated vectorized\nloads and arithmetic instructions. A set of higher registers is\nassigned to be loaded with inputs and weights from different\ninput channels. In addition, this block is executed multiple\ntimes depending on the hyperparameters of the convolutional\nlayer, such as the kernel size.\n\n@) ReLU Block. The third block adds the biases and per-\nforms the activations. For the FP16 implementation, the two\npartial sums of the accumulator need to be combined before\nadding the bias and calculating the ReLU. Conversely, in\nthe INT8 implementation, the partial sum results are already\nsummed into a single register, but there is a need to convert\nthis number to a floating-point prior to adding the bias and\napplying ReLU. Unlike the FP16 implementation, which uses\nhalf precision throughout the computation, the INT8 imple-\nmentation converts the integer to a single-precision floating-\npoint number.\n\nInit block Convolutional block ReLU block\n\n50\nss 50\n0 0 0\n50\n50 100\n2 0 ~_Hhi\u2014e \u2014\n\u2014100\n\n0.0 05 1.0 15 2.0 2.5 3.0\n\nFigure 3: Raw trace of the whole operation on the GPU of\nJetson Nano. The two convolutional layers (light pink and\nyellow) are clearly separated in the traces. Additionally, the\nCUDA device-to-host memory copy (light purple) is also\nclearly visible in the end of the trace.\n\nMatching Instruction Block to Traces. Figure 3 shows the\nelectromagnetic emanations of the Jetson Nano GPU during\nthe execution of a CNN with two convolutional layers. Since\neach layer is mapped to a separate CUDA function call by\nthe framework, it can be observed that there is a clear separa-\n\ntion between layers. Additionally, the convolution results are\ncopied back to memory, which is a separate CUDA call and\nis visible in the trace.\n\nIn addition, the three blocks of the implementations can\nalso be identified. The first highlighted part shows the init\nblock in the first layer. The second highlighted block cor-\nresponds to the convolutional block where the partial sums\nare computed. The third highlighted segment corresponds\nto the calculation of the bias addition and ReLU output, re-\npeated four times for different sets of registers. Note that these\ninstruction blocks are also separated by synchronization in-\nstructions, which are also visible in the trace as the amplitude\nof the EM signal drops close to 0 between the blocks.\n\n4.2 Identifying Partial Sums\n\nAs no single GPU instruction can process an arbitrary number\nof arguments, the code typically processes these convolutions\nas a sequence of partial sums. An example of a naive way of\ncomputing Equation 1| is by sequentially computing partial\nsums s; using the formula:\n\nJ\nSj =D wieri = sj tw yx (i<p-q) @)\n\ni=]\n\nWith this formula, if we assume we know sj; and x;, we\ncan guess w; and use the guess to compute s;. We then apply a\nleakage model to the computed sj, e.g., the Hamming distance\nbetween s;_; and s;, and search for correlations between the\nmodel and observed leakage. A high correlation indicates that\nthe guess of w; is correct.\n\nThe challenge is that there are multiple ways in which the\ncode can compute Equation |. In particular, the implementa-\ntion can change the order of computing the sum, and it can\nalso use vector operations to combine multiple additions into\na single operation. As our attack relies on knowing the previ-\nous partial sum to guess the weight, we must know how the\nsum is computed.\n\nIn this section, we delve into the implementations of convo-\nlutions with two reduced-precision implementations used by\nthe TensorRT framework: INT8 and FP16. Reduced-precision\nimplementations, such as these, are typically used during in-\nference because they reduce both the latency and the memory\nuse compared to single-precision implementations, which\nare typically used in the training phase. Our choice of INT8\nand FP16 targets the larger and more challenging parameter\nsizes in reduced-precision techniques [30, 43, 45, 50, 50], and\ncovers both integer and floating-point parameters. Here we\ndescribe the high-level design of the implementations. See\nAppendix A for further technical details.\n\nFP16 Convolution. The FP16 implementation uses the\nHFMA2 instruction, which performs two half-precision fused-\nmultiply-adds in parallel. It operates on 32-bit registers, each\nholding two half-precision floating-point numbers. It first\n",
                    "visual_analysis": "Caption: Figure 3 shows the raw trace of a convolutional block on the GPU with two convolution layers using FERI16 as reference.\nFigure 4 depicts matching instruction blocks for two different CNNs executed by Jetson Nano and Jetson TX2 (NVIDIA) respectively, each implementing an implementation similar to that used in Figure~\\ref{fig:rawtrace}. The goal of this figure is not only to show how the instructions are mapped on the GPUs but also provide a comparison with respect to architecture differences. As mentioned earlier, the two CNNs have identical architectures and implement different convolutional operations such as ReLU or Convolution Block (CB) respectively.\n"
                }
            },
            {
                "page_7": {
                    "ocr_text": "computes the products of two pairs of numbers in matching\nhalves of two registers and then adds the results to the match-\ning halves of a third register, which acts as an accumulator.\nThis, basically, splits the convolution computation across the\ntwo channels, which are summed at the end. Our leakage\nmodel targets each partial sum (16 bits) that is written into\nthis accumulator register.\n\nINTS8 Convolution. The INT8 implementations of convo-\nlutional and dense layers use the IDP . 4A instruction to per-\nform a 4-way dot product and accumulation operation, de-\npicted in Figure 9. The instruction first multiplies the elements\nin matching channels in two registers. It then sums the results\nand adds them to a third register, which stores a signed 32-bit\naccumulator.\n\nIn many cases, not all four channels of the dot product\ncontain data. Specifically, when the number of input channels\nis three, the channels of a single input point are convolved\nwith the matching weights. On the other hand, when the input\nconsists of a single channel, two input points are convolved\nduring each operation. Values for channels that are not used\nare set to zero and do not affect the result of the instruction.\n\nUnlike the HFMA2 instruction, the sum depends on all of\nthe weights that are convolved by the IDP . 4A instruction.\nThus, the attack needs to target all of the weights that are used\nby a single instruction.\n\n4.3 Localizing Partial Sums\n\nSection 4.1 demonstrates that we can identify the high-level\noperations in the trace, including the layer processing and\nthe main steps of their computation. In Section 4.2, we show\nhow we find the partial sums that leak specific weights. In\nthis section, we complete the profiling and identify the trace\nlocations that leak each partial sum and, consequently, the\nweights. Our approach employs TVLA [24] to find statistical\nevidence of leakage.\n\nTVLA. To determine the time points in the traces where\nvalues of the partial sums s; leak, we apply fixed vs. random\nTVLA to find statistical evidence for leakage. Specifically, we\ncollect two sets of traces. In the \u201cfixed\u201d set, we set the target\nweight to a fixed non-zero value, all other weights to zero, and\nall of the input to fixed randomly chosen values and collect\nmultiple traces of performing inference with the model. For\nthe \u201crandom\u201d set, we similarly collect multiple traces, but for\neach trace, we randomly select the value of the target weight.\nAll other weights and inputs are set to the same fixed values\nas in the \u201cfixed\u201d set. We then compare the distribution of\nvalues of each time point across the two sets using Welch\u2019s\nt-test. As common when performing side-channel attacks, if\nthe absolute t-value is above 4.5, i.e., |t] > 4.5, we mark the\npoint as a potential leakage.\u00b0\n\n\u00aeTo verify that TVLA does not yield false positives, we check the cor-\nresponding intermediate-value correlations of a few weights to confirm the\nleakage. For example, when a computation followed Equation 3, we used\n\nweight leakage\n\nAmplitude\n\nbias leakage\n\nSamples x10\u00b0\n\nFigure 4: Result of fixed vs. random TVLA for the first weight\n(top) and the bias (bottom) in FP16 convolution with 30K and\n37K traces, respectively. The middle depicts an example trace.\nThe dashed red line indicates the 4.5 threshold.\n\nFigure 4 shows the results of fixed vs. random TVLA for\nthe first weight in the kernel in the first layer for FP16 convo-\nlution on the Jetson Nano. Leakage is clearly evident at the\nstart of the convolutional block, around sample 25 000, where\nthe t value goes above 4.5. Repeating the process for all of\nthe weights of the kernel allows us to identify the time point\nfor each weight at which they leak.\n\nTo create the random set for fixed vs. random TVLA, we\nneed to repeatedly modify the target weight to a random value.\nThe TensorRT framework supports changing the weights of\nmodels, but a new CUDA context [4] has to be created every\ntime the weights of a network are changed. This contrasts\nwith the fixed case, where an application is not required to\ncreate a new CUDA context for every inference operation.\nHowever, as TVLA requires changing the weights, we cannot\navoid this.\n\n4.4 Bias Leakage\n\nFor the bias, the final partial sum co,; or the output of the\nactivation cy has to leak. Similarly, to detect leakage corre-\nsponding to the bias, we apply fixed vs. random bias TVLA.\n\nFigure 4 also shows the results of TVLA for the bias of\nthe kernel in the first layer for FP16 convolution. A much\nclearer leakage is present for the bias than for the weights in\nthe convolution operation.\n\nAfter establishing where parameters leak, with the help of\nelastic alignment, we use static alignment on the raw traces\nat these points as static alignment produced higher individual\nTVLA peaks as well as correlation for the attack.\n\nHD(sj;~1,5;). All these experiments confirmed the TVLA results.\n",
                    "visual_analysis": "Caption: Fixated vs. random TVLA for the first weight and (the bias) in FP16 convolution with 30K and 37K traces respectively.\nAnswer: This figure shows that there is a significant difference between fixed vs. random training weights when tested on both Convolutional Neural Network data using FP16 precision compared to floating point, specifically the results from FP32 for TVLA were superior in terms of accuracy with lower mean absolute error (MAE) values across all metrics such as MAE, Recall@50%, and F-measure. This suggests that fixed weights are more robust against quantization errors associated with using less precision than floating point during training on neural networks used to process Convolutional Neural Network data like the MNIST dataset which uses 8-bit integer representations of pixels (gray-level intensities). It highlights an advantage for TVLA in scenarios where it is applied, emphasizing its effectiveness under conditions typically encountered when working directly with raw pixel values."
                }
            },
            {
                "page_8": {
                    "ocr_text": "4.5 CEMA Implementation in CUDA\n\nIn order to calculate correlation in CEMA, the covariances\nof populations have to be calculated, which can be done with\ntwo-pass algorithms. However, for large datasets, two-pass\nalgorithms are inefficient as the algorithm makes two itera-\ntions on the dataset. Therefore, one-pass algorithms have been\ndeveloped to estimate these statistics in large datasets [47]. In\na one-pass algorithm, the statistics can be updated when new\ndata points are added to the dataset. These algorithms also\nmake it possible to combine the statistics from subsets of the\ndataset to estimate the whole dataset\u2019s statistics. This means\nthat the statistics of each subset can be calculated in parallel,\nfurther speeding up the CEMA attack.\n\nHowever, the attack has even more aspects that can be par-\nallelized, such as the candidate and sample levels. There are\npublicly available multithreaded implementations such as the\nJISCA library [8] for CPU, but these cannot fully parallelize\nthe attack and become slow for large datasets. Since we have a\nlarge number of candidates with FP16 weights, we decided to\nimplement CEMA for neural networks in CUDA, parallelized\non three levels: dataset, candidate, and sample.\n\nOur implementation in CUDA launches a_ three-\ndimensional grid of thread blocks with dimensions:\n(candidates /2, chunks, samples) = (17765, chunks,32). For\nour use case, the number of candidates and samples are fixed\nat 35530 and 32, respectively. Threads in the same warp\nwork on the same two candidates (in parallel, due to double\nthroughput with FP16) but correlate them with different\nsamples. The implementation also parallelizes CEMA on a\ndata set level, by setting chunks > 1. The number of chunks\ncan vary, but setting it to 10 already gives good results in our\nuse case with millions of traces. The optimal number may be\nlower or higher depending on the GPU hardware.\n\nWe benchmark the multithreaded JISCA implementation on\nan AMD Ryzen 7950X CPU vs. our CUDA implementation\non a 3080 Nvidia RTX GPU. Overall, the speedup compared\nto JISCA is at least x5 and typically x 10 when the dataset\nconsists of millions of traces.\n\n5 Parameter Extraction Results\n\nIn this section, we demonstrate the generality of our parameter\nextraction framework, shown in Figure 1, on a real-world\nCNN architecture by targeting the baseline EfficientNet [59]\nto extract a kernel from its first two convolutional layers. Note\nthat none of the previous works have demonstrated parameter\nextraction from real-world CNN architectures.\n\nIn these experiments, after using the profiling information\nfrom Section 4, we use the partial sums to extract FP16 and\nINT8 weights on the Jetson Nano and Jetson Orin Nano,\nrespectively. In addition, we also analyze the impact of batch\nsize on the attack.\n\n5.1 FP16 Parameter Extraction\n\nFollowing [13], we restrict the search space of the FP16\nweights to [\u20145,5], as most of the parameters of trained CNNs\nreside in this range. To verify this, we looked at the parameters\nof large real-world architectures, such as our target Efficient-\nNetBO, trained on ImageNet. With 16-bit floats, there are\n35 330 possible candidates in this range.\n\nIn the experiment, the parameters of the target architectures\nare initialized randomly. Observe that while we attack in an\niterative fashion, the trace acquisition needs to be executed\nonly once with random known inputs. We target the FP16\npartial sums in each layer to extract weights and biases.\n\n5.1.1. Convolutional Layer\n\nIn this experiment, we show the results of our framework by\nsuccessfully extracting FP16 parameters from the first 2 con-\nvolutional layers of the baseline EfficientNet [59] architecture\nby targeting the partial sums in the layer.\u2019 Both the HW and\nHD leakage models prove to be exploitable in recovering\nweights and biases from the layers. Targeting the partial sums\nallows us to use a divide-and-conquer approach by reducing\ncomplexity, as only one FP16 weight must be extracted from\na kernel at a time.\n\nWeight Extraction. For the weights, we target the partial\nsums sj; of the 2D convolution and extract the weights of a\nkernel one by one. First, we are able to extract weights using\nthe HW leakage model. As shown in Figure 5a and Figure 5b,\nwe successfully recover the third weight of a kernel in the\nfirst layer. For the second layer, Figure 5e and Figure 5f show\nthe key ranking and correlation for the second weight in the\nsecond layer. In addition, Figure 5g and Figure 5h show the\nkey ranking and correlation for the third weight in the second\nlayer, demonstrating that our attack extends to larger layers\nas well.\n\nThe HD(sj-1,5;) leakage model targeting a register up-\n\ndate can also be exploited to recover weights: Figure 5c and\nFigure 5d show the key rankings and correlations for the 9th\nweight in the first layer of the large architecture using HD.\nThe attack works similarly for the other weights. As shown in\nFigure 5c, the convergence behavior for HD is different from\nthat for HW as it starts to converge slower.\nBias Extraction. Similarly to the weights, we are also able\nto use HD leakage model to extract the bias: we targeted the\nregister update from Csym tO Cour: HD(Csum,Cout). Figure 6e\nand Figure 6f show the results for the bias in the first layer.\nThe key rank drops quickly and converges to key rank 0 in\n5 million traces. Overall, there is a significant variance in\nthe number of required traces to recover individual weights\nand biases, but an upper bound of 20 million traces proves\nsufficient in our experiments.\n\n7The convolutional layers in this architecture do not have biases. There-\nfore, we alter the architecture so that the first layer has biases in the kernels\nto demonstrate the bias extraction on this architecture as well.\n",
                    "visual_analysis": "The two-column table titled \"5 Parameter Extraction Results\" compares the speedup on a NVIDIA RTX GPU compared to an AMD Ryzen 7970 XRX CPU for various parameter extraction methods and kernel depths (64K). The results are presented in terms of execution time, with higher values indicating faster performance. For example, at Kernel Depth =15:28 and Parameter Extraction Method = CNN1/Convolutional, the speedup is shown as 0.97x for both the GPU and CPU compared to a previous benchmark.\nThe figure \"Figure 6d\" depicts two kernel layers with varying depth (K=3) over different depths of traces extracted from various datasets like CNN1, EFF14, and CudaNet. The y-axis represents speedup factors on an AMD Ryzen RTX GPU in microseconds per trace compared to a previous benchmark using the same parameters.\nThe paragraph \"5 Parameter Extraction Results\" explains that the generality of their parameter extraction framework is demonstrated by showing results on various datasets including CNNs trained on ImageNet, CudaNet models with different depths and weights, as well as kernel layers. They mention varying speeds based on dataset size (e.g., 1768/2) for two-kernel scenarios using the same parameters.\nThe paragraph \"5 Parameter Extraction Results\" includes a comparison of execution time across various datasets such as CNN1, EFF14, and CudaNet models trained with different depths and weights. The results show varying speeds depending on factors like dataset size (e.g., 1768/2 for two-kernel scenarios)."
                }
            },
            {
                "page_9": {
                    "ocr_text": "30000 4\n\n20000 4\n\nas\n\nI\n\n10000 4\n\nkey\n\n04\n\n0.0 0.5 Lo 15 2.0\nnumber of traces x10\u00b0\n(a) Key rank vs. number of traces using HW for the third\nweight in the first layer with value of 0.8223.\n\n30000\n\nrank\n\n20000\n\nkey\n\n10000\n\n0\n\n0.0 0.5 10 15 7\nnumber of traces x10!\n(c) Key rank vs. number of traces using HD for the ninth\nweight in the first layer with value of -0.7705.\n\n10000 4\n\nkey rank\n\n0.0 0.5 LO 15 2.0 2.5\nnumber of traces x 10\u00b0\n(e) Key rank vs. number of traces for s; for the second\nweight in the second layer with the value of -0.5137.\n\n15000 4\n\u201cs\n= 10000 4\n= 5000 4\n0 1 T T T T\n0 2 4 6 :\nnumber of traces x 10\u00b0\n\n(g) Key rank vs. number of traces for s; for the third weight\nin the second layer with the value of -0.6406.\n\n\u2014\u2014 incorrect candidates\n\n0.02 correct candidate\n\n0.01\n\ncorrelation\n\n0.00\n\n0.0 05 L0 15 2.0\nnumber of traces x 10\u00b0\n(b) Correlation vs. number of traces using HW for the third\nweight in the first layer with value of 0.8223.\n\n0.0105 (Tt incorrect candidates\n\n\u2014\u2014 correct candidate\n\ncorrelation\n>\nSo\nao\n\n0.000\n\n0.0 0.5 1.0 1.5 -\nnumber of traces x10!\n\n(d) Correlation vs. number of traces using HD for the ninth\nweight in the first layer with value of -0.7705.\n\n\u2014 incorrect candidates\n0.010 5 :\n\u2014 correct candidate\n\ncorrelation\n\n0.000 +\n\n0.0 0.5 1.0 15 20 25\nnumber of traces \u00ab10\u00b0\n\n(f) Correlation vs. number of traces for s; for the second\n\nweight in the second layer with the value of -0.5137.\n\n\u2014 incorrect candidates\n\n9.010 7 correct candidate\n\n2\nr)\nS\na\nL\n\ncorrelation\n\n0.000 5\n\nnumber of traces x10\u00b0\n\n(h) Correlation vs. number of traces for s; for the third\nweight in the second layer with the value of -0.6406.\n\nFigure 5: Key ranks and correlations of the different FP16 weights in the first and second layer on the Jetson Nano.\n\n5.2 INTS8 Parameter Extraction\n\nFor INT8 weights, we demonstrate our parameter extraction\nframework on the first convolutional layer with the same Ef-\nficientNetBO configuration as in the FP16 case, also with\ndifferent batch sizes. Similarly, we target the partial sums in\nthe INT8 convolution to extract the weights. In addition, we\nprovide results on dense layer parameter extraction with INT8\nweights.\n\n10\n\n5.2.1. Convolutional Layer\n\nWeight Extraction. In this case, the partial sums can de-\npend on multiple weights, depending on the number of input\nchannels. In this experiment, the number of input channels to\nthe first layer is one, so the partial sums depend on exactly\none 8-bit weight. If the number of input channels is larger\nthan 1, as is the case in subsequent layers, then the target\nintermediate values depend on multiple weights, increasing\n",
                    "visual_analysis": "The graphs and plots displayed include various scatter plots with lines connecting the points to demonstrate trends and correlations between two variables: the number of traces (or \"traces\") versus key rank for different layers within a neural network model using hardware acceleration techniques like HP (high performance) and HD (high density). The x-axis in these graphs typically represents the logarithmic scale of the number of traces, ranging from 0.1 to approximately \\(2 \\times 10^{6}\\), with some plots extending up to \\(4 \\times 10^7\\) or even higher.\n\nThe y-axis for most charts shows \"key rank,\" which is a measure related to the performance or accuracy metric used within neural network training, ranging from zero (indicating no correlation) to over 300k. The lines plotted in different colors distinguish between incorrect candidate keys and correct ones as identified by certain algorithms.\n\nSpecifically:\n- **Figure 4(a)**: Shows \"Key rank vs. number of traces using HW for the third layer\" with a value close to 8223.\n- **Figure 5**: Displays key ranks across various layers (first, second) compared to their corresponding correlation values on Jetson Nano hardware.\n\n**Table and Chart Description**:\nThe table at the bottom titled \"INT8 Parameter Extraction\" lists two columns under different headers: one for weights extracted with an efficient framework using FP16 configuration (\"FP16 case\") and another for a Jetson Nano-based extraction where partial sums are considered. It shows correlations between traces for each layer, indicating values like -0.7523 or 849.\n\n**Figure Caption**: \"Figure 5: Key ranks and correlations of the different FP16 weights in the first and second layers on the Jetson Nano.\"\n\nThe context provided suggests a detailed analysis within neural network optimization using hardware acceleration techniques to improve performance, particularly for convolutional layers."
                }
            },
            {
                "page_10": {
                    "ocr_text": "4000 4\n4\n2. 2000 4\na)\n04\nT T T T\n0.0 0.5 1.0 15\nnumber of traces x10\u2018\n(a) Key rank vs. number of traces for ReLU.\n20000 4\n2\n\u00a9 10000 4\n04\n\n1.25 1.50\n\n1.00 .\nx 10\u2018\n\n0.50 0.75\nnumber of traces\n\n0.00 0.25\n\n(c) Key rank vs. number of traces for Cour.\n\n10000 4\n\nkey rank\n\n5000 4\n\n0 1 2 3 4 5\n\nnumber of traces x 10\u00b0\n\n(e) Key rank vs. number of traces when Csym is overwritten\nwith Cout +\n\n\u2014\u2014 incorrect candidates\n\n0.010 4\n\ncorrect candidate\n\n0.000 +\n\nT T\n1.0 150\nnumber of traces x10\u2018\n\n(b) Correlation vs. number of traces for ReLU.\n\n\u2014\u2014 incorrect candidates\ncorrect candidate\n\n0.005\n\ncorrelation\n\n0.000\n\nT\n\nT T T T\n0.00 0.50 0.75 1.00 1.25\n\nnumber of traces\n\n0.25 1.50\n\nx10\"\n\n(d) Correlation vs. number of traces for Coy.\n\noe incorrect candidates\ncorrect candidate\n= 0.005 4\n0.000 4\n\n0 1 2 3 4 5\n\nnumber of traces x 10\u00b0\n\n(f) Correlation vs. number of traces when C5, is overwrit-\nten with Coy.\n\nFigure 6: Key ranks and correlations of the FP16 biases with different leakage models on the Jetson Nano.\n\nthe complexity of the attack.\n\nSimilarly to the FP16 case, we extract the weights in a ker-\nnel one by one by targeting the partial sums in the convolution.\nFigure 7a and Figure 7a show the results for 4th weight in\na kernel, while Figure 7c and Figure 7d show the results for\nthe 5th weight in a kernel, respectively. In our experiments,\non average, 300K traces were enough for the correct key can-\ndidates to reach key rank 0. Both the HW and HD leakage\nmodels are exploitable to recover individual weights.\n\nImpact of Batch Size. The batch size does influence the\nnumber of traces required to extract weights but not signifi-\ncantly, as 500K traces are sufficient to reach key rank 0. The\ndifference between smaller and larger batch sizes is the num-\nber of executing threads. For instance, the first convolutional\nlayer is executed with 512 and 7 232 threads for batch sizes of\n1 and 16, respectively. Even though there are 14 times more\nthreads for batch size 16, the number of executing threads\nphysically in parallel is limited, e.g., the warp schedulers still\nonly issue instructions for at most 32 threads at a time. There-\nfore, on smaller GPUs, the main impact of batch size might\n\n11\n\nbe only the linear increase in execution time. On the other\nhand, a larger batch size provides even more partial sums in\none trace, and a horizontal attack [18] exploiting this might\nenhance the efficacy of the attack. We leave analyzing this for\nfuture work.\n\n5.2.2 Dense Layer\n\nWe present parameter extraction results of a neural network\ncontaining a single dense layer with INT8 weights running\non the Jetson Orin Nano. In the experiments, our target dense\nlayer has 512 nodes, and the input size s of the layer is 784.\nTherefore, each node has 784 weights associated with it. Our\nleakage modeling experiments for dense layers discovered\nthat, similarly to convolutional layers, the HW and HD leak-\nage models are both viable for mounting a successful pa-\nrameter extraction attack. However, these exploitable leakage\nmodels depend on multiple, not just one, 8-bit weights be-\ncause the input size to dense layers is significantly larger than\n4 in real-world applications. Therefore, most or all partial sum\nresults depend on four weights, i.e., we face a complexity of\n",
                    "visual_analysis": "The figure displays several plots and a table that correlate key rank with various metrics such as the number of traces for ReLU activation function (ReLU), correlation between candidates during trace extraction using different methods on Jetson Nano GPU, impact on batch size in training models like FP16 biases and FP32 weight distributions. The top left plot shows how 'Key Rank' varies against a logarithmic scale with respect to the number of traces for ReLU activation function within 80K iterations (x-axis) and corresponds to key rank versus the number of traces, suggesting an inverse relationship where increasing trace counts correlate less frequently with high-ranking keys. The top right plot indicates how 'Correlation' varies across different trace numbers in a logarithmic scale compared to incorrect candidates during ReLU extraction for FP16 biases on Jetson Nano GPU; it shows that correlation increases up until around 5,000 traces and then diminishes as the number of traces rises. Below these plots are two more similar correlations with an x-axis labeled \"number of traces\" scaled from zero to ten thousand times (x10^7), showing a decline in correct-correlated candidates against incorrect ones within FP16 biases context, emphasizing correlation decay beyond certain thresholds.\n\nThe bottom-left plot discusses 'Key rank vs. number of traces' for ReLU with c_out = 50K iterations; it highlights that as the trace count increases on an exponential scale from zero to ten thousand times (x10^7), key ranks tend to be lower, suggesting a possible relationship between high trace counts and low-key rankings in this context.\n\nThe bottom-right plot shows how 'Correlation' varies against different traces numbers with c_out = 50K iterations; here the correlation peaks around fifty-thousand traces before diminishing significantly beyond it. The middle section of plots examines key ranks for FP16 biases using various leakage models on Jetson Nano GPU, indicating that incorrect candidates often result in lower or equal key rankings compared to correct ones.\n\nLastly, there's a table and graph discussing 'Batch Size' impact where the target batch size affects computational efficiency; higher batch sizes increase execution times while decreasing accuracy. The plot indicates results for different layers (128k) on Jetson Nano GPU with varying leakages like 3x10^4 or even larger, reflecting real-world application complexities and outcomes.\n\nOverall, these visual aids collectively illustrate how key ranks correlate across various computational environments such as trace counts during ReLU extraction in neural networks, correlation metrics for incorrect vs. correct candidates using different methods on Jetson Nano GPU with varying batch sizes impacting machine learning models' efficiency and accuracy within specific contexts like FP16 biases training or larger-scale data handling."
                }
            },
            {
                "page_11": {
                    "ocr_text": "key rank\n\nT T T T T\n0 200000 = 400000 = 600000 = 800000\n\nnumber of traces\n\n(a) Key rank vs. number of traces with HD leakage model\nfor the fourth weight in the convolutional kernel.\n\nkey rank\n\n100\n\n0 200000 400000 600000 800000\nnumber of traces\n\n(c) Key rank vs. number of traces with HD leakage model\nfor the fifth weight in the convolutional kernel.\n\n100 4\n\nkey rank\n\n0 200000 400000 600000\nnumber of traces\n\n(e) Key rank vs. number of traces with HW leakage model\nfor the 8th weight in the dense layer.\n\n100\n\n0 200000 400000 ~~. 690000\nnumber of traces\n(g) Key rank vs. number of traces with HD leakage model\nfor the 8th weight in the dense layer.\n\n[oc\n\ncorrect candidate\n\n\u2014\u2014\u2014\n\n200000 400000 600000 800000\n\nnumber of traces\n\n=\nS\nis)\n\ncorrelation\n\nS\nSo\nm\n\n\u2014\u2014 incorrect candidates\n\n(b) Correlation vs. number of traces with HD leakage\nmodel for the fourth weight in the convolutional kernel.\n\n0.02 + 7 ; F\n\u2014\u2014 incorrect candidates\n\n\u2014\u2014 correct candidate\n\n0 200000 400000 600000 800000\nnumber of traces\n\n(d) Correlation vs. number of traces with HD leakage\nmodel for the fifth weight in the convolutional kernel.\n\n\u2014 incorrect candidates\n\n\u2014\u2014 correct candidate\n\n0.00 4\n\n0 200000 400000 600000\nnumber of traces\n\n(f) Correlation vs. number of traces with HW leakage\nmodel for the 8th weight in the dense layer.\n\ncorrect candidate\n\n0.02\n\n0.\n\n0.00\n\n200000 400000 00000\n\nnumber of traces\n\ncorrelation\n\n=\n\n0.03 4 \u2014\u2014 _ incorrect candidates\n\n(h) Correlation vs. number of traces with HD leakage\nmodel for the 8th weight in the dense layer.\n\nFigure 7: Key ranks and correlations of the different INT8 weights of convolutional and dense layers on the Jetson Orin Nano.\n\n32 bits with the INT8 data type. To combat this issue, we ap-\nply the chosen-input attack mentioned earlier by setting three\nchannels to 0 so that the registers that hold the input values\ncontain only | non-zero input. Therefore, the final result will\nonly depend on one 8-bit weight. Since all the nodes in the\nlayer receive the same inputs, the chosen input attack does not\nrequire a new trace-set for every node separately. Therefore,\nthe attack is independent of the size of the layer.\n\n12\n\nFigure 7e and Figure 7f show the results with HW leakage\nmodel while Figure 7g and Figure 7h show the results with\nHD leakage model for the 8th weight in the first node. In\nour experiments, on average, 300K traces were enough for\nthe correct key candidates to reach key rank 0, similar to the\nconvolutional layer. In addition, both the HW and HD leakage\nmodels are exploitable to recover individual weights, which is\nunsurprising since the same instruction is used for the layers.\n",
                    "visual_analysis": "Figure 7 displays various key ranks and correlations across different INT8 weights of convolutional and dense layers on the Jetson Orin Nano GPU architecture. The x-axis represents the number of traces used for analysis, ranging from zero to over a million.\n\n(a) \"Key rank vs. Number of Traces with HD Leakage Model\" illustrates how incorrect candidates (black line) compare against correct candidates' key ranks as we increase trace counts.\n(b) \"Correlation vs. Number of Traces with HD Leakage Model,\" showing correlation between traces and model weights, indicates a strong negative correlation for the fourth weight in both convolutional and dense layers.\n(c) Similar to figure (b), it displays correlations across different weights but on separate graphs: fifth weight in convolutional kernel and eighth weight in Dense Layer Convolutional Kernel.\n(d) \"Correlation vs. Number of Traces with HW Leakage Model,\" again shows a strong negative correlation for the second, fifth, and eighth weights.\n\n(a): The line chart fluctuates as trace count increases but generally trends downward from incorrect candidates (black).\n(b), (c), & (d): Correlation lines show that more traces lead to higher accuracy correlations.\n(e) \"Key rank vs. Number of Traces with HW Leakage Model\" again shows how key ranks change, focusing on the dense layer.\n\n(g) and (h) continue this trend for eighth weight in Dense Layer Convolutional Kernel, maintaining a strong correlation as trace counts increase significantly.\n\nThe text below each graph explains that 32-bit operations were conducted using INT8 data. The chosen attack methods are detailed; Figure 7a-b show results with HW leakage models while Figures 7c-e illustrate HD leakages and their impact on key rank vs. number of traces for different layers. Figures 7g-h display correlations across various weights in dense layer convolutional kernels.\n\nKey insights from this figure include the critical role trace count plays in accurately identifying correct candidates, particularly with HW leakages where correlation values reach near-1 as more traces are utilized. This suggests that increasing computational resources (traces) enhances model accuracy and reliability under specific leakage conditions across different layers of a neural network GPU architecture like Jetson Orin Nano."
                }
            },
            {
                "page_12": {
                    "ocr_text": "Jetson Nano\n\nJetson Orin Nano\n\nweights data type FP16 INT8\nbias data type FP16 FP32\n# of extracted layers 2 1\n\n# of extracted parameters 9564 316\nbias extraction w~ -\n\n# of max req. traces 20M 3M\n\nTable 2: Parameter extraction results for different GPUs for\nthe EfficientNetBO architecture.\n\n5.2.3. Attacking Deeper Layers.\n\nIn deeper layers with INT8 weights, each partial sum de-\n\npends on four 8-bit weights as the number of input channels\n\nis larger than four. Generally, this increases the complexity\nof the attack to guessing four weights at a time, so 2\u00b0 candi-\ndates, which is computationally expensive. However, if the\nattacker has enough computing resources, the attack can still\nbe mounted by attacking 32 bits.\n\nHowever, the complexity can be reduced in two similar\nways:\n\n1. Collect traces with random inputs and choose only those\nwhere the inputs to deeper layers in specific channels are\nzero.\n\n2. Collect traces with chosen inputs so deeper layers receive\nzero inputs in specific channels.\n\nRandom Inputs. This approach implies that the attacker has\n\nto collect significantly more traces to have 300K traces with\n\nthe desired 0-value inputs in specific channels. However, the\ninputs in DNNs are usually centered around zero, and some\nof their quantized values are exactly 0. More importantly, the\n\nReLU activation function greatly enhances sparsity in the\n\ninputs due to setting negative values to 0.\n\nChosen Inputs. In this approach, the attacker has to solve\n\nsystems of linear equations to generate inputs to the first layer\n\nsuch that the inputs to deeper layers are 0, similarly to [25].\n\nThe number of collected traces is significantly less than that\n\nof random inputs, but whether this approach works for deeper\n\nlayers in practice is still to be determined. Nevertheless, quan-\ntization and the ReLU activation function also enhance this\napproach as these factors make the solutions space larger.\n\n5.2.4 Parameter extraction comparisons\n\nThe summary of parameter extraction of the target Efficient-\nNetBO architecture on different GPUs is presented in Table 2.\nFor the Jetson Nano with FP16 parameters, the first two con-\nvolutional layers can be extracted with a complexity of 2!\u00b0\nfor each parameter. However, a full model extraction is still\ncomputationally expensive as the model contains millions\nof parameters. In this case, the parameters required at most\n20M traces for successful extraction. Observe that the number\n\n13\n\nof traces should not increase even if we attack more layers\nbecause the attack does not make assumptions about layers\u2019\ninputs.\n\nFor the Jetson Orin Nano with INT8 weights, we were only\nable to extract the weights of the first layer as the parameters\nin the second layer have a complexity of 23? candidates due to\nhow partial sums depend on 4 weights. In addition, the biases\nare represented as 32-bit floats that also provide a complexity\nof 2\u00b0? candidates. Although the number of required traces\nis at most 3M for the first layer weights, subsequent layers\nrequire most likely more as there are 23\u201d candidates. Overall,\neven with FP16 parameters, a full model extraction requires\ncomputing resources that are beyond our capabilities but not\nbeyond a well-founded commercial or state attacker.\n\n6 Discussion\n\n6.1 Approaching Additional platforms\n\nOur approach is demonstrated on CUDA-enabled GPUs, but\nwe expect that our methodology is applicable to other plat-\nforms as well. An attack on another platform would also\nstart with a profiling phase where the target architecture\u2019s\nimplementation is reverse-engineered on a low level. This is\nnecessary to identify and localize partial sums allowing for\nparameter extraction. After the profiling phase, the attacker is\nequipped with the appropriate leakage models and locations\nto extract the parameters. We expect that there would be some\ndifferences with respect to the exact intermediate values that\nwould need to be targeted, but the approach should be similar.\n\n6.2 Desktop/Datacenter GPUs\n\nAgainst large GPUs, our attack can be extended and is likely\nto be more expensive depending on the target neural network\u2019s\nsize. One key difference between the Jetsons and desktop/-\ndatacenter GPUs is the number of streaming multiprocessors.\nIdeally, for a large GPU, an attacker would first locate all the\nSMs of the target GPU. Afterward, each SM could be scanned\nwith an EM probe to see if there is any activity of interest.\nIf the target neural network is extremely large and saturates\nall the SMs during inference, multiple probes may be used to\ncover all of them and collect traces in parallel for each SM.\nTherefore, the equipment and overall cost is higher the more\nSMs the target GPU has.\n\n6.3 Limitations\n\nWhile we demonstrated parameter extraction on multiple\nGPUs, the coverage is still limited:\n\nConcurrent Applications. GPUs are able to handle and\nschedule concurrently CUDA functions from multiple appli-\ncations. This might introduce noise, but it depends on the re-\nquired resources for each CUDA function. Suppose a CUDA\n",
                    "visual_analysis": "Table 2: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | FP16 |\n| --- | --- |\n| bias data type | FP16 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n| bias extract | \u2713 |\n| # of max req. traces | 3M |\n\nTable 2: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | FP16 INT8 ORIN Nano Jetson Orin Nano |\n| --- | --- |\n| bias data type | FP16 FP32 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 3M |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n| Bias extract | \u2713 |\n| # of max req. traces | 3M |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 ORI |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of extracted parameters | 9564 |\n\nTable: Parameter extraction results for different GPUs for the EfficientNetB0 architecture.\n| weights data type | Jetson Nano ORIN Nano |\n| --- | --- |\n| bias data type | FP16 INT8 |\n| # of extracted layers | 2 |\n| # of"
                }
            },
            {
                "page_13": {
                    "ocr_text": "function takes up most of the GPU resources (e.g., shared\nmemory, registers, etc.). In that case, a different CUDA func-\ntion will only be scheduled after all the thread blocks in the\nprevious CUDA function finished execution [3]. Therefore,\non GPUs with few SMs, this is less likely to be an issue as\nthe GPU\u2019s resources are already saturated due to the large\ncomputational demand of DNN inference.\n\nBias Extraction in INT8 Implementations We demon-\nstrated the extraction of biases from convolutional layers for\nFP16 implementations but not INT8 implementations. These\nimplementations only use INT8 for the weights but not for\nthe bias. The bias in these implementations has FP32 data\ntype, which is beyond our computing capabilities to extract.\n\n6.4 Mitigation\n\nTraditional ways to contain electromagnetic emanation, such\nas proper shielding or introducing noise to decrease the Signal-\nto-Noise ratio, could alleviate the problem [42]. Specifically\nagainst parameter extraction, one of the possible countermea-\nsures, which is also mentioned in the CSI-NN paper [13], is\nshuffling [61] the order of multiplications in the layers, which\ncan make it significantly harder for an adversary to recover\nthe weights. Additionally, masking [19, 48] can also decou-\nple the side-channel measurements and the processed data.\nHowever, this comes at the price of execution speed, which\nmight not be desired in real-time systems. Specifically for\nconvolution, the registers containing the results of the partial\nsums can be initialized with the bias of the kernel instead of\ninitializing them with zeros. This would prompt an adversary\nto mount a CEMA attack where the correct b+ w, pair has\nto be recovered first. The complexity of this attack would be\n32 bits due to 16 bits of complexity for the weight and bias\nseparately in the FP16 case. However, in the INT8 case, the\nbias is a single-precision float, so it cannot be used to initialize\nthe accumulator registers.\n\nClock Side Parameter\nAuthor Platform (MHz) channel datatype\nBatina, et al. [13] MCU 20, 84 EM FP32\nDubet, et al. [21] FPGA 24 Power Binary\nYoshida, et al. [64] FPGA 25 Power INT8\nRegazzoni, et al. [53] | FPGA N/AS EM Binary\nYli-Mayry, et al. [63] | FPGA N/A8 EM Binary\nLi, et al. [37] FPGA 25 Power INT8\nJoud, et al. [31] MCU 100 EM FP32\nGongye et al. [25] FPGA 320 EM INT8\nBarraCUDA GPU 625,920 EM INT8, FP16\n\nTable 3: Comparison with related work.\n\n6.5 Related Work\n\nTo the best of our knowledge, no previous work has been\nable to extract the parameters of neural networks on GPU us-\n\ning physical side-channel. Previous works have demonstrated\nparameter extraction on microcontrollers and FPGAs using\npower or EM side channel, as shown in Table 3. In addition,\nthese attacks were performed on neural networks with binary\nparameters [21, 53, 63], 8-bit parameters [13, 25, 37, 64] or\n32-bit parameters [13, 31]. Our work demonstrates parame-\nter extraction of 8- and 16-bit parameters. Furthermore, our\nwork presents a CEMA attack on weights where the number\nof cores and the clock frequency at these cores operate are\nsignificantly larger than in related works. The large number\nof cores, with almost 1GHz clock frequency, presents a chal-\nlenge in both the measurement and attack stages. Given that\nGPUs are the backbone of AI, it is of utmost importance to\nassess the resilience of GPU accelerated workloads against\nweight extraction attacks, a task our research addresses.\n\n7 Conclusions\n\nIn this work, we analyzed the GPUs of Nvidia Jetson Nano\nand Nvidia Jetson Orin Nano, commonly chosen platforms\nfor real-world neural network implementations, for resilience\nagainst side-channel attacks that aim to extract the weights\nof the target NN. First, we find multiple vulnerable points\nwhere the GPUs leak information about the parameters of the\ntarget DNN. Subsequently, we demonstrate the extraction of\nweights and biases of convolutional and dense layers. Overall,\nthe neural network implementations of Nvidia\u2019s TensorRT\nframework are vulnerable to parameter extraction using EM\nside-channel attack despite the networks running in a highly\nparallel and noisy environment. Protecting their implementa-\ntions in security or privacy-sensitive applications remains an\nopen problem.\n\nAcknowledgments\n\nThis research was supported by: an ARC Discovery Project\nnumber DP210102670; the Deutsche Forschungsgemein-\nschaft (DFG, German Research Foundation) under Germany\u2019s\nExcellence Strategy - EXC 2092 CASA - 390781972; Ai-\nSecTools (VJO2010010); PROACT project of Dutch Research\nAgenda (NWA.1215.18.014) and Netherlands Organisation\nfor Scientific Research (NWO); TTW PREDATOR project\n19782 (NWO).\n\nReferences\n\n[1] https://www.langer-emv.de/en/product /mfa-\nactive-1mhz-up-to-6-ghz/32/mfa-r-0-2-\n75-near-field-micro-probe-1-mhz-up-to-1-\n\nghz/854. Accessed: 2022-01-25.\n\n8The clock frequency is not disclosed in these attacks, but it is at most\n800MHz as both attack XILINX ZYNQ chip [12].\n",
                    "visual_analysis": "Table 3 depicts the comparison with related work for GPU architectures and includes various parameters such as clock speed (MHz), side channel parameter type, whether it is power, EM based on a certain reference like BarraCUDA / FP16/FP8, etc., alongside their respective authors' names. The table lists notable examples including MCU 20,84 by Batina et al.[3], which was an early work in the field of side channel attacks and countermeasures using CUDA functions.\n\nTable 5 shows comparison with related works for the GPU architectures discussed. It highlights six researchers and their respective methodologies: Batina & colleagues' examination of EM on different platforms, Dube's study of mitigation methods involving electromagnetic radiation or noise to decrease signal-to-noise ratio (SNR), Yoshida et al.'s research into a countermeasure in the CSI-PPN array using side channel analysis for resilience against real-world neural network implementations. It also mentions Fujikawa & colleagues' exploration on Nvidia Jetson Nano, and Liang's work with GPU architectures.\n\nThe table presents varying levels of complexity, as seen from different numbers such as 19 or more in one study versus fewer studies like the BarraCUDA / FP16/FP8 approach. The methodologies range widely across real-world scenarios to specialized security research frameworks for resilience against attacks.\n\nTable 3 is a comprehensive list showcasing various works on GPU architectures and related side-channel attack techniques, providing insight into their complexity levels and underlying approaches in countering these threats within computing environments."
                }
            },
            {
                "page_14": {
                    "ocr_text": "[2]\n\n[3\n\n\u201c4\n\n[4]\n\n[5]\n\n[6\n\nfo)\n\n[7\n\n\u2014)\n\n[8]\n\n[9]\n\n[10]\n\n[11]\n\n[12]\n\n[13]\n\n[14]\n\n[15]\n\nhttps://www.langer-emv.de/en/product/rf-\npassive-30-mhz-up-to-3-ghz/35/rf-b-0-3-3-\nh-field-probe-mini-30-mhz-up-to-3-ghz/17.\nAccessed: 2023-03-25.\n\nhttps: //developer.download.nvidia.com/CUDA/\ntraining/StreamsAndConcurrencyWebinar.pdf.\n\nAccessed: 2022-11-30.\n\nCuda _ Context. https://docs.nvidia.com/\ncuda/cuda-c-programming-guide/index.html#\ncontext. Accessed: 2022-09-30.\n\nCUDA programming model. https://docs.\nnvidia.com/cuda/cuda-c-programming-\nguide/index.html#programming-model.\n\ncessed: 2022-09-30.\n\nAc-\n\ncuobjdump. https://docs.nvidia.com/cuda/\ncuda-binary-utilities/#usage. Accessed: 2022-\n09-30.\n\nJetso orin nano module. https: //developer.nvidia.\ncom/downloads/assets/embedded/secure/\njetson/orin_nano/docs/jetson_orin_nano_ds.\nAccessed: 2024-03-15.\n\nJisca. https://github.com/Riscure/Jlsca. Ac-\n\ncessed: 2022-09-30.\n\nNVIDIA Jetson Nano. https://developer.nvidia.\ncom/embedded/jetson-nano-developer-kit. Ac-\n\ncessed: 2022-09-30.\n\nSIMT _ architecture. https://docs.nvidia.\ncom/cuda/cuda-c-programming-guide/#simt-\u2014\narchitecture. Accessed: 2022-09-30.\n\nTegra X1 System-On-Chip. http://international.\ndownload.nvidia.com/pdf/tegra/Tegra-X1-\nwhitepaper-vl.0.pdf. Accessed: 2022-09-30.\n\nZYNQ Data Sheet. https: //docs.xilinx.com/v/u/\nen-US/ds187-XC7Z010-XC7Z020-Data-Sheet. Ac-\ncessed: 2022-09-30.\n\nLejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan\nPicek. CSI-NN: Reverse engineering of neural network\narchitectures through electromagnetic side channel. In\nUSENIX Security, pages 515-532, 2019.\n\nEric Brier, Christophe Clavier, and Francis Olivier. Cor-\nrelation power analysis with a leakage model. In CHES,\npages 16-29, 2004.\n\nKumar Chellapilla, Sidd Puri, and Patrice Simard. High\nperformance convolutional neural networks for docu-\nment processing. In Frontiers in Handwriting Recogni-\ntion, 2006.\n\n15\n\n[16]\n\n[17]\n\n[18]\n\n[19]\n\n[20]\n\n[21]\n\n[22]\n\n[23]\n\n[24]\n\n[25]\n\n[26]\n\n[27]\n\n[28]\n\nLukasz Chmielewski and L\u00e9o Weissbart. On reverse\nengineering neural network implementation on GPU. In\nAIHWS, pages 96-113, 2021.\n\nFrancois Chollet. Xception: Deep learning with depth-\nwise separable convolutions. In CVPR, pages 1251-\n1258, 2017.\n\nChristophe Clavier, Benoit Feix, Georges Gagnerot,\nMyl\u00e9ne Roussellet, and Vincent Verneuil. Horizontal\ncorrelation analysis on exponentiation. In JCICS, pages\n46-61, 2010.\n\nJean-S\u00e9bastien Coron and Louis Goubin. On Boolean\nand arithmetic masking against differential power anal-\nysis. In CHES, pages 231-237, 2000.\n\nJosef Danial, Debayan Das, Santosh K. Ghosh, Arijit\nRaychowdhury, and Shreyas Sen. Scniffer: Low-cost,\nautomated, efficient electromagnetic side-channel sniff-\ning. IEEE Access, 8:173414\u2014173427, 2019.\n\nAnuj Dubey, Rosario Cammarota, and Aydin Aysu.\nMaskednet: The first hardware inference engine aim-\ning power side-channel protection. In HOST, pages\n197-208, 2020.\n\nFiirkan Elibol, Ugur Sarac, and Isin Erer. Realistic eaves-\ndropping attacks on computer displays with low-cost\nand mobile receiver system. In EUSIPCO, pages 1767\u2014\n1771, 2012.\n\nKunihiko Fukushima. Visual feature extraction by a mul-\ntilayered network of analog threshold elements. IEEE\nTrans. Syst. Sci. Cybern., 5(4):322-333, 1969.\n\nBenjamin Jun Gilbert Goodwill, Josh Jaffe, Pankaj Ro-\nhatgi, et al. A testing methodology for side-channel\nresistance validation. In NIST non-invasive attack test-\ning workshop, volume 7, pages 115-136, 2011.\n\nCheng Gongye, Yukui Luo, Xiaolin Xu, and Yunsi Fei.\nSide-channel-assisted reverse-engineering of encrypted\nDNN hardware accelerator IP and attack surface explo-\nration. In JEEE S&P, 2024.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 770-778, 2016.\n\nZhang Hongxin, Huang Yuewang, Wang Jianxin,\nLu Yinghua, and Zhang Jinling. Recognition of electro-\nmagnetic leakage information from computer radiation\nwith SVM. Computers & Security, 28(1-2):72-76, 2009.\n\nKurt Hornik, Maxwell Stinchcombe, and Halbert White.\nMultilayer feedforward networks are universal approxi-\nmators. Neural Networks, 2(5):359-366, 1989.\n",
                    "visual_analysis": "The image shows a list of references with corresponding bibliographic information formatted according to APA style guidelines. Each entry includes the authors' names, title of the work, publication year, and URL if available.\n\nReferences are listed as follows:\n\n1. [2] A hyperlink pointing to an online document hosted by Langer-Emv.\n2. [3] Information about a software development download for CUDA training materials from NVIDIA's website.\n3. [4] Link to documentation on CUDA context using the provided link.\n4. [5] URL directing readers to information regarding the programming model used in CUDA.\n5. A collection of books or articles, each with its authors' names and publication years.\n\nThe list includes titles such as \"Reverse engineering neural network implementation,\" \"Deep learning for high-performance computing,\" among others, suggesting a focus on computer science topics related to artificial intelligence, deep learning, GPU programming, etc. Each reference entry is formatted consistently with the standard APA style including in-text citations (not shown) and full bibliographic entries like this one.\n\nIt's important to note that some URLs are truncated or partially redacted for privacy reasons, hence not all links can be accessed directly from the image provided here.\n(Note: The images contain text which appears as a list of references with their corresponding author names, titles, publication years, and URL.)"
                }
            },
            {
                "page_15": {
                    "ocr_text": "[29]\n\n[30]\n\n[31]\n\n[32]\n\n[33]\n\n[34]\n\n[35]\n\n[36]\n\n[37]\n\n[38]\n\n[39]\n\n[40\n\n=\n\n[41]\n\nPeter Horvath, Lukasz Chmielewski, Leo Weissbart,\nLejla Batina, and Yuval Yarom. CNN architecture ex-\ntraction on edge GPU. In AJHWS, 2024.\n\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. Binarized neural net-\nworks. In NeurIPS, 2016.\n\nRapha\u00e9l Joud, Pierre-Alain Mo\u00e9llic, Simon Ponti\u00e9, and\nJean-Baptiste Rigaud. A practical introduction to side-\nchannel extraction of deep neural network parameters.\nIn CARDIS, pages 45-65, 2022.\n\nPaul Kocher, Joshua Jaffe, and Benjamin Jun. Differen-\ntial power analysis. In CRYPTO, pages 388-397, 1999.\n\nPaul C. Kocher. Timing attacks on implementations\nof Diffie-Hellman, RSA, DSS, and other systems. In\nCRYPTO, pages 104-113, 1996.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural\nnetworks. NeurIPS, 25:1097\u2014-1105, 2012.\n\nMarkus G. Kuhn and Ross J. Anderson. Soft Tempest:\nHidden data transmission using electromagnetic emana-\ntions. In International Workshop on Information Hiding,\npages 124-142, 1998.\n\nNikolay Laptev, Jason Yosinski, Li Erran Li, and Slawek\nSmyl. Time-series extreme event forecasting with neural\nnetworks at uber. In JCML, pages 1-5, 2017.\n\nGe Li, Mohit Tiwari, and Michael Orshansky. Power-\nbased attacks on spatial DNN accelerators. ACM Jour-\nnal on Emerging Technologies in Computing Systems,\n18(3):1-18, 2022.\n\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in\nnetwork. arXiv preprint arXiv: 1312.4400, 2013.\n\nLi Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth,\nJie Chen, Xinwang Liu, and Matti Pietikadinen. Deep\nlearning for generic object detection: A survey. Inter-\nnational journal of computer vision, 128(2):261-318,\n2020.\n\nZhuoran Liu, Niels Samwel, L\u00e9o Weissbart, Zhengyu\nZhao, Dirk Lauret, Lejla Batina, and Martha Larson.\nScreen gleaning: A screen reading TEMPEST attack\non mobile devices exploiting an electromagnetic side\nchannel. NDSS, 2021.\n\nHenrique Teles Maia, Chang Xiao, Dingzeyu Li, Eitan\nGrinspun, and Changxi Zheng. Can one hear the shape\nof a neural network?: Snooping the GPU via magnetic\nside channel. In USENIX Security, pages 4383-4400,\n2022.\n\n16\n\n[42]\n\n[43]\n\n[44]\n\n[45]\n\n[46]\n\n[47]\n\n[48]\n\n[49]\n\n[50]\n\n[51]\n\n[52]\n\nStefan Mangard, Elisabeth Oswald, and Thomas Popp.\nPower analysis attacks: Revealing the secrets of smart\ncards, volume 31. Springer Science & Business Media,\n2008.\n\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Mar-\nius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd, John\nKamalu, Naveen Mellempudi, Stuart Oberman, Moham-\nmad Shoeybi, Michael Siu, and Hao Wu. FP8 formats\nfor deep learning. arXiv preprint arXiv:2209.05433,\n2022.\n\nAmir Moradi, Bastian Richter, Tobias Schneider, and\nFrancois-Xavier Standaert. Leakage detection with the\nx2-test. IACR Transactions on Cryptographic Hardware\nand Embedded Systems, pages 209-237, 2018.\n\nSharan Narang, Gregory Diamos, Erich Elsen, Paulius\nMicikevicius, Jonah Alben, David Garcia, Boris Gins-\nburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, et al. Mixed precision training. In JCLR,\n2017.\n\nDaniel W Otter, Julian R Medina, and Jugal K Kalita.\nA survey of the usages of deep learning for natural lan-\nguage processing. IEEE transactions on neural net-\nworks and learning systems, 32(2):604\u2014624, 2020.\n\nPhilippe Pierre P\u00e9bay. Formulas for robust, one-pass\nparallel computation of covariances and arbitrary-order\nstatistical moments. Sandia Report SAND2008-6212,\nSandia National Laboratories, 2008.\n\nEmmanuel Prouff and Matthieu Rivain. Masking against\nside-channel attacks: A formal security proof. In Euro-\ncrypt, pages 142-159, 2013.\n\nHendrik Purwins, Bo Li, Tuomas Virtanen, Jan Schliiter,\nShuo-Yiin Chang, and Tara Sainath. Deep learning\nfor audio signal processing. IEEE Journal of Selected\nTopics in Signal Processing, 13(2):206-219, 2019.\n\nJerry Quinn and Miguel Ballesteros. Pieces of eight:\n8-bit neural machine translation. In NAACL-HLT (3),\npages 114-120, 2018.\n\nJean-Jacques Quisquater and David Samyde. Elec-\ntromagnetic analysis (EMA): measures and counter-\nmeasures for smart cards. In E-smart, pages 200-210,\n2001.\n\nSyama Sundar Rangapuram, Matthias W. Seeger, Jan\nGasthaus, Lorenzo Stella, Yuyang Wang, and Tim\nJanuschowski. Deep state space models for time se-\nries forecasting. NeurIPS, 2018.\n",
                    "visual_analysis": "Title: [29] Peter Horvath, Lukasz Chmielowski, Elio Weissbart, and Thomas Popp.\n\nBody Text:\nLenja Batina, and Yuval Yarom. CNN architecture exponentiation on edge GPU. In AIHWS, 2024.\n\nSubtitle: Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.\nBinarized neural network works. In NeurIPS, 2016.\n\nTitle: [30] Paul C. Kocher, Joshua Jaffe, and Benjamin Jun.\nDifferentiation power analysis. In CRyPTO, pages 388\u2013397, 1999.\n\nBody Text:\nTiming attacks on implementations of Diffie-Hellman, RSA, SSDS, etc. systems. In CRYPTO, pages 104\u2013113, 1996.\nAlex Krizhevsky, Ilya Sutsker, and Geoffrey E Hinton. ImageNet classification with deep convolutional neural networks, NeurIPS, 25:1097-1108, 2012.\n\nTitle: [4] Markus G. Kahn\nBody Text:\nand Ross J. Anderson. Soft Tempest.\nHidden data transmission using electromagnetic emanations. In International Workshop on Information Hiding, pages 124\u2013142, 1998.\n\nTitle: [36] Nikolay Laptev,\nYosinshi Karii, Li Erran Liu, and Slawek Smyl. Systems.\nBody Text:\nSmiy. In ICRLM, page forecasting with neural networks at uber."
                }
            },
            {
                "page_16": {
                    "ocr_text": "[53] Francesco Regazzoni, Shivam Bhasin, Amir Alipour,\nThab Alshaer, Furkan Aydin, Aydin Aysu, Vincent\nBeroulle, Giorgio Di Natale, Paul D. Franzon, David\nH\u00e9ly, Naofumi Homma, Akira Ito, Dirmanto Jap,\nPriyank Kashyap, Ilia Polian, Seetal Potluri, Rei Ueno,\nElena Ioana Vatajelu, and Ville Yli-Mayry. Machine\nlearning and hardware security: Challenges and oppor-\ntunities. In ICCAD, pages 141:1\u2014141:6, 2020.\n\n[54] Alaa Sagheer and Mostafa Kotb. Time series forecast-\n\ning of petroleum production using deep Istm recurrent\n\nnetworks. Neurocomputing, 323:203-213, 2019.\n\n[55] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim\n\nJanuschowski. Deepar: Probabilistic forecasting with\n\nautoregressive recurrent networks. International Jour-\n\nnal of Forecasting, 36(3):1181-1191, 2020.\n\n[56] Tobias Schneider and Amir Moradi. Leakage assess-\n\nment methodology. In CHES, pages 495-513, 2015.\n\n[57] David Silver, Thomas Hubert, Julian Schrittwieser, Ioan-\n\nnis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanc-\n\ntot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\net al. Mastering chess and shogi by self-play with a gen-\neral reinforcement learning algorithm. arXiv preprint\n\narXiv:1712.01815, 2017.\n\n[58] Karen Simonyan and Andrew Zisserman. Very deep con-\n\nvolutional networks for large-scale image recognition.\n\narXiv preprint arXiv: 1409, 1556, 2014.\n\n[59] Mingxing Tan and Quoc Le. EfficientNet: Rethinking\n\nmodel scaling for convolutional neural networks. In\n\nICML, pages 6105-6114, 2019.\n\n[60] Jasper G. J. van Woudenberg, Marc F. Witteman, and\n\nBram Bakker. Improving differential power analysis by\n\nelastic alignment. In C7-RSA, pages 104-119, 2011.\n\n[61\n\n\u2014\n\nNicolas Veyrat-Charvillon, Marcel Medwed, St\u00e9phanie\nKerckhof, and Frangois-Xavier Standaert. Shuffling\nagainst side-channel attacks: A comprehensive study\nwith cautionary note. In Asiacrypt, pages 740-757,\n2012.\n\n[62] Mengjia Yan, Christopher W. Fletcher, and Josep Torrel-\nlas. Cache telepathy: Leveraging shared resource attacks\nto learn DNN architectures. In USENIX Security, pages\n2003-2020, 2020.\n\n[63] Ville Yli-Mayry, Akira Ito, Naofumi Homma, Shivam\nBhasin, and Dirmanto Jap. Extraction of binarized neu-\nral network architecture and secret parameters using\nside-channel information. In ISCAS, pages 1-5, 2021.\n\n17\n\n[64] Kota Yoshida, Takaya Kubota, Shunsuke Okura, Mit-\nsuru Shiozaki, and Takeshi Fujino. Model reverse-\nengineering attack using correlation power analysis\nagainst systolic array based neural network accelerator.\nIn ISCAS, pages 1-5, 2020.\n\nConvolution implementation details\n\nListing 1: FP16 convolution code snippet\n\nHFMA2 RO, R94, R109, RO;\nHFMA2 R3, R92, R108, R3;\nLDS.U.128 R116, [R88+0x210];\n\nR1 R2\n\n(ADD }\n\nRO\n\nFigure 8: HFMA2 instruction operation with two input chan-\nnels.\n\nIn this section, we show how the computation of partial\n\nsums differs based on the representation of the parameters\nand how this influences the attack. Specifically, the FP16 and\nINTS8 data types have their own specialized instructions where\nthe GPU registers are akin to vector registers.\nFP16 convolution. As demonstrated in Listing 1, FP16-based\nconvolution uses the HFMA2 instruction, which performs two\nhalf-precision fused-multiply-adds in parallel. As illustrated\nin Figure 8, the instruction takes three input registers, each\nis a two-lane vector. It multiplies the values in the matching\nchannels of two registers, adds the result to the matching\nlanes of the third, and stores the result in the output register.\nIn all instances we have seen, a single register is used as an\naccumulator, where the multiplication result is added to it.\nOur leakage model targets each partial sum (16 bits) that is\nwritten into this accumulator register.\n\nListing 2: INT8 convolution code snippet\n\nIDP.4A.S8.S8 R62, R71, R83, R62;\nIDP.4A.S8.S8 R59, R69, R82, R59Y;\nLDS .128 R64, [R97+0x200];\n\nINTS8 convolution. Listing 2 shows an example of code\nused in the INT8 implementation. The implementation uses\n\n",
                    "visual_analysis": "Figure 6: HFMA2 instruction operation with two input channels.\nIn the given figure, we see a representation of how partial computation is done based on different representations of parameters used during training for artificial neural networks (ANNs). The diagram shows that the FP16 and FP32 data types are utilized in their respective specialized instructions to calculate vector registers. This graphical depiction helps understand where GPU registers serve as storage locations, which is a key concept when learning about ANN architectures like HFMA2 instruction operation involving two input channels."
                }
            },
            {
                "page_17": {
                    "ocr_text": "Figure 9: IDP.4A instruction operation with 4 input channels.\n\nthe IDP . 4A instruction to perform a 4-way dot product and\naccumulate operation, depicted in Figure 9. The instruction\nfirst multiplies the elements in matching lanes in two registers.\nIt then sums the results and adds them to a third register, which\nstores a signed 32-bit integer. As in the FP16 case, in all uses\nwe have seen, IDP. 4A uses a single register as input and\noutput numbers.\n\nTo extract weights, our leakage model targets the 32-bit\npartial sum (s, in Figure 9) stored in the accumulator register.\nAs the result depends on all four input lanes, the complexity of\nthe CEMA attack grows to up to 32 bits. In practice, the code\noften uses less than four lanes. Specifically, when processing\ngreyscale images, convolutions tend to use only one lane,\nwhereas when processing color images, convolutions use three\nlanes, one for each color channel.\n\nUsing the INT8 representation on the Orin Nano, Tensor\nCore implementations use the IMMA (Integer Matrix Mul-\ntiply and Accumulate) instruction. IMMA works on a warp\nlevel, meaning the threads in the warp must cooperate, but\nits execution is similar to IDP. 4A on a thread level. The\nimplementations we experiment with in this paper do not use\nthe IMMA instruction. However, the techniques we develop\nfor parameter extraction are directly applicable to it as well.\nDense layer. The implementation of dense layers follows\nthe same design as convolutional layers. In these implementa-\ntions, each accumulator register holds the partial sums of the\nweighted sum of a node in the layer. Similarly to INT8 con-\nvolutional implementations, the dense layer implementation\nuses the IDP . 4A instruction to calculate the partial sums.\nConsequently, as in the convolutional, there is a need to guess\n32 bits, except in edge cases, where some channels are set to\nzero.\n\n18\n",
                    "visual_analysis": "The figure labeled \"Figure 9: IDP.4A instruction operation with 4 input channels\" shows a schematic of the Instruction Data Pointer (IDP) for an integer multiplexor using four-bit product and accumulate operations depicted in Figure 9 of another document. It illustrates how these instructions are performed by calculating dot products, summing results into registers, storing signed-32 bit integers, and utilizing single use output numbers from a register.\n\nThe schematic includes the following components:\n1. **Registers**: R1, R2, R0.\n2. **Memory Blocks (W1 to W4)**: Each block has four input channels labeled as i1 to i4.\n3. **MUL Operations**: Multiplication operations between different memory blocks and registers are shown with colored lines indicating the multiplications performed.\n\nThe text explains:\n- The IDP uses 4-way dot product instructions in a register store structure, similar to Fig.9.\n- It sums results from matching lanes into three registers: an integer adder (32-bit), two accumulator registers for storing integers and products respectively, and output numbers.\n- Leakage models target the sum of partial products per cycle.\n\nThe complexity increases with four input channels as:\n- The CEMA instruction's code grows up to 32 bits in practice due to higher memory usage compared to other instructions like convolutions."
                }
            }
        ],
        "total_pages": 18
    }
}